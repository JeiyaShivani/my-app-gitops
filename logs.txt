
==> Audit <==
┌─────────┬────────────────────────────────────────┬──────────┬──────┬─────────┬─────────────────────┬─────────────────────┐
│ COMMAND │                  ARGS                  │ PROFILE  │ USER │ VERSION │     START TIME      │      END TIME       │
├─────────┼────────────────────────────────────────┼──────────┼──────┼─────────┼─────────────────────┼─────────────────────┤
│ start   │ --driver=docker                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 01:10 CDT │                     │
│ start   │ --driver=docker                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 01:49 CDT │                     │
│ start   │ --driver=docker                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 03:01 CDT │ 12 Oct 25 03:05 CDT │
│ service │ node-api-service                       │ minikube │ js   │ v1.37.0 │ 12 Oct 25 03:33 CDT │                     │
│ start   │ --driver=docker                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 12:04 CDT │                     │
│ delete  │                                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 12:11 CDT │ 12 Oct 25 12:11 CDT │
│ start   │ --driver=docker --cpus=2 --memory=2048 │ minikube │ js   │ v1.37.0 │ 12 Oct 25 12:11 CDT │ 12 Oct 25 12:12 CDT │
│ service │ node-api-service                       │ minikube │ js   │ v1.37.0 │ 12 Oct 25 12:15 CDT │                     │
│ start   │                                        │ minikube │ js   │ v1.37.0 │ 12 Oct 25 23:33 CDT │ 12 Oct 25 23:34 CDT │
│ ip      │                                        │ minikube │ js   │ v1.37.0 │ 13 Oct 25 02:20 CDT │ 13 Oct 25 02:20 CDT │
│ service │ node-api-service                       │ minikube │ js   │ v1.37.0 │ 13 Oct 25 02:21 CDT │                     │
│ start   │                                        │ minikube │ js   │ v1.37.0 │ 13 Oct 25 03:43 CDT │ 13 Oct 25 03:44 CDT │
│ service │ node-api-service                       │ minikube │ js   │ v1.37.0 │ 13 Oct 25 03:46 CDT │                     │
└─────────┴────────────────────────────────────────┴──────────┴──────┴─────────┴─────────────────────┴─────────────────────┘


==> Last Start <==
Log file created at: 2025/10/13 03:43:15
Running on machine: js-VirtualBox
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1013 03:43:15.655864    4835 out.go:360] Setting OutFile to fd 1 ...
I1013 03:43:15.657177    4835 out.go:413] isatty.IsTerminal(1) = true
I1013 03:43:15.657181    4835 out.go:374] Setting ErrFile to fd 2...
I1013 03:43:15.657184    4835 out.go:413] isatty.IsTerminal(2) = true
I1013 03:43:15.657329    4835 root.go:338] Updating PATH: /home/js/.minikube/bin
I1013 03:43:15.663970    4835 out.go:368] Setting JSON to false
I1013 03:43:15.666084    4835 start.go:130] hostinfo: {"hostname":"js-VirtualBox","uptime":300,"bootTime":1760344695,"procs":233,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.14.0-29-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"dd67df0d-353f-422e-972e-f775278e247d"}
I1013 03:43:15.667938    4835 start.go:140] virtualization: vbox guest
I1013 03:43:15.676855    4835 out.go:179] 😄  minikube v1.37.0 on Ubuntu 24.04 (vbox/amd64)
I1013 03:43:15.686891    4835 notify.go:220] Checking for updates...
I1013 03:43:15.697581    4835 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1013 03:43:15.717328    4835 driver.go:421] Setting default libvirt URI to qemu:///system
I1013 03:43:18.822579    4835 docker.go:123] docker version: linux-28.3.3:Docker Engine - Community
I1013 03:43:18.822751    4835 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1013 03:43:23.223676    4835 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.400881822s)
I1013 03:43:23.224634    4835 info.go:266] docker info: {ID:a3b0add2-ae5a-489e-9412-6d30ab345889 Containers:4 ContainersRunning:3 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:32 OomKillDisable:false NGoroutines:50 SystemTime:2025-10-13 03:43:23.098810103 -0500 CDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-29-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4006166528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:js-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.26.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.1]] Warnings:<nil>}}
I1013 03:43:23.224831    4835 docker.go:318] overlay module found
I1013 03:43:23.252513    4835 out.go:179] ✨  Using the docker driver based on existing profile
I1013 03:43:23.268610    4835 start.go:304] selected driver: docker
I1013 03:43:23.268628    4835 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 03:43:23.268818    4835 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1013 03:43:23.270745    4835 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1013 03:43:24.053083    4835 info.go:266] docker info: {ID:a3b0add2-ae5a-489e-9412-6d30ab345889 Containers:4 ContainersRunning:3 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:32 OomKillDisable:false NGoroutines:50 SystemTime:2025-10-13 03:43:23.949336194 -0500 CDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-29-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4006166528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:js-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.26.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.1]] Warnings:<nil>}}
I1013 03:43:24.057221    4835 cni.go:84] Creating CNI manager for ""
I1013 03:43:24.075592    4835 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1013 03:43:24.075860    4835 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 03:43:24.096569    4835 out.go:179] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1013 03:43:24.116538    4835 cache.go:123] Beginning downloading kic base image for docker with docker
I1013 03:43:24.133461    4835 out.go:179] 🚜  Pulling base image v0.0.48 ...
I1013 03:43:24.188406    4835 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1013 03:43:24.188552    4835 preload.go:146] Found local preload: /home/js/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1013 03:43:24.188673    4835 cache.go:58] Caching tarball of preloaded images
I1013 03:43:24.189427    4835 preload.go:172] Found /home/js/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1013 03:43:24.189461    4835 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1013 03:43:24.189797    4835 profile.go:143] Saving config to /home/js/.minikube/profiles/minikube/config.json ...
I1013 03:43:24.196813    4835 image.go:81] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1013 03:43:24.440669    4835 image.go:100] Found docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1013 03:43:24.440688    4835 cache.go:147] docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1013 03:43:24.441352    4835 cache.go:232] Successfully downloaded all kic artifacts
I1013 03:43:24.441399    4835 start.go:360] acquireMachinesLock for minikube: {Name:mk049652724668be9d894285788dad0b3d9cc578 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1013 03:43:24.441835    4835 start.go:364] duration metric: took 402.797µs to acquireMachinesLock for "minikube"
I1013 03:43:24.441879    4835 start.go:96] Skipping create...Using existing machine configuration
I1013 03:43:24.441886    4835 fix.go:54] fixHost starting: 
I1013 03:43:24.443672    4835 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 03:43:24.613542    4835 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1013 03:43:24.613636    4835 fix.go:138] unexpected machine state, will restart: <nil>
I1013 03:43:24.648795    4835 out.go:252] 🔄  Restarting existing docker container for "minikube" ...
I1013 03:43:24.654516    4835 cli_runner.go:164] Run: docker start minikube
I1013 03:43:29.577627    4835 cli_runner.go:217] Completed: docker start minikube: (4.923065075s)
I1013 03:43:29.577848    4835 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 03:43:29.953324    4835 kic.go:430] container "minikube" state is running.
I1013 03:43:29.968414    4835 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 03:43:30.265822    4835 profile.go:143] Saving config to /home/js/.minikube/profiles/minikube/config.json ...
I1013 03:43:30.270863    4835 machine.go:93] provisionDockerMachine start ...
I1013 03:43:30.273590    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:43:30.586507    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:43:30.622319    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:43:30.622345    4835 main.go:141] libmachine: About to run SSH command:
hostname
I1013 03:43:30.633398    4835 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1013 03:43:33.685663    4835 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46034->127.0.0.1:32768: read: connection reset by peer
I1013 03:43:36.711682    4835 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46040->127.0.0.1:32768: read: connection reset by peer
I1013 03:43:42.790795    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1013 03:43:42.790817    4835 ubuntu.go:182] provisioning hostname "minikube"
I1013 03:43:42.790910    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:43:43.698689    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:43:43.699940    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:43:43.699957    4835 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1013 03:43:45.638615    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1013 03:43:45.638743    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:43:47.641567    4835 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (2.002747973s)
I1013 03:43:47.641675    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:43:47.642091    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:43:47.642170    4835 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1013 03:43:48.927332    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1013 03:43:48.927481    4835 ubuntu.go:188] set auth options {CertDir:/home/js/.minikube CaCertPath:/home/js/.minikube/certs/ca.pem CaPrivateKeyPath:/home/js/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/js/.minikube/machines/server.pem ServerKeyPath:/home/js/.minikube/machines/server-key.pem ClientKeyPath:/home/js/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/js/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/js/.minikube}
I1013 03:43:48.927598    4835 ubuntu.go:190] setting up certificates
I1013 03:43:48.927614    4835 provision.go:84] configureAuth start
I1013 03:43:48.937330    4835 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 03:43:49.756818    4835 provision.go:143] copyHostCerts
I1013 03:43:49.782403    4835 exec_runner.go:144] found /home/js/.minikube/ca.pem, removing ...
I1013 03:43:49.784728    4835 exec_runner.go:203] rm: /home/js/.minikube/ca.pem
I1013 03:43:49.785681    4835 exec_runner.go:151] cp: /home/js/.minikube/certs/ca.pem --> /home/js/.minikube/ca.pem (1066 bytes)
I1013 03:43:49.791052    4835 exec_runner.go:144] found /home/js/.minikube/cert.pem, removing ...
I1013 03:43:49.791074    4835 exec_runner.go:203] rm: /home/js/.minikube/cert.pem
I1013 03:43:49.791173    4835 exec_runner.go:151] cp: /home/js/.minikube/certs/cert.pem --> /home/js/.minikube/cert.pem (1111 bytes)
I1013 03:43:49.801125    4835 exec_runner.go:144] found /home/js/.minikube/key.pem, removing ...
I1013 03:43:49.801152    4835 exec_runner.go:203] rm: /home/js/.minikube/key.pem
I1013 03:43:49.801270    4835 exec_runner.go:151] cp: /home/js/.minikube/certs/key.pem --> /home/js/.minikube/key.pem (1675 bytes)
I1013 03:43:49.806186    4835 provision.go:117] generating server cert: /home/js/.minikube/machines/server.pem ca-key=/home/js/.minikube/certs/ca.pem private-key=/home/js/.minikube/certs/ca-key.pem org=js.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1013 03:43:51.813958    4835 provision.go:177] copyRemoteCerts
I1013 03:43:51.814106    4835 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1013 03:43:51.814166    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:43:52.601302    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:43:53.357471    4835 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.543320931s)
I1013 03:43:53.357561    4835 ssh_runner.go:362] scp /home/js/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1066 bytes)
I1013 03:43:53.940322    4835 ssh_runner.go:362] scp /home/js/.minikube/machines/server.pem --> /etc/docker/server.pem (1168 bytes)
I1013 03:43:55.101157    4835 ssh_runner.go:362] scp /home/js/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1013 03:43:56.945950    4835 provision.go:87] duration metric: took 8.018319008s to configureAuth
I1013 03:43:56.945973    4835 ubuntu.go:206] setting minikube options for container-runtime
I1013 03:43:57.005618    4835 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1013 03:43:57.008385    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:43:57.957104    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:43:57.958150    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:43:57.958169    4835 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1013 03:44:00.904465    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1013 03:44:00.904485    4835 ubuntu.go:71] root file system type: overlay
I1013 03:44:00.904736    4835 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1013 03:44:00.915530    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:01.885488    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:44:01.889839    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:44:01.890050    4835 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1013 03:44:03.696356    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1013 03:44:03.696669    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:04.315197    4835 main.go:141] libmachine: Using SSH client type: native
I1013 03:44:04.348789    4835 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1013 03:44:04.348820    4835 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1013 03:44:05.414949    4835 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1013 03:44:05.473721    4835 machine.go:96] duration metric: took 35.202280387s to provisionDockerMachine
I1013 03:44:05.473753    4835 start.go:293] postStartSetup for "minikube" (driver="docker")
I1013 03:44:05.473772    4835 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1013 03:44:05.474886    4835 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1013 03:44:05.480410    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:05.976647    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:06.942154    4835 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.464688122s)
I1013 03:44:06.942250    4835 ssh_runner.go:195] Run: cat /etc/os-release
I1013 03:44:07.032941    4835 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1013 03:44:07.032981    4835 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1013 03:44:07.032995    4835 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1013 03:44:07.033006    4835 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1013 03:44:07.033024    4835 filesync.go:126] Scanning /home/js/.minikube/addons for local assets ...
I1013 03:44:07.070129    4835 filesync.go:126] Scanning /home/js/.minikube/files for local assets ...
I1013 03:44:07.074080    4835 start.go:296] duration metric: took 1.60029289s for postStartSetup
I1013 03:44:07.074262    4835 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1013 03:44:07.074330    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:07.449115    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:07.977869    4835 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1013 03:44:08.141881    4835 out.go:203] 
W1013 03:44:08.166322    4835 out.go:285] 🧯  Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity). You can pass '--force' to skip this check.
W1013 03:44:08.167407    4835 out.go:285] 💡  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
W1013 03:44:08.175314    4835 out.go:285] 🍿  Related issue: https://github.com/kubernetes/minikube/issues/9024
I1013 03:44:08.193620    4835 out.go:203] 
I1013 03:44:08.202299    4835 fix.go:56] duration metric: took 43.760393528s for fixHost
I1013 03:44:08.202332    4835 start.go:83] releasing machines lock for "minikube", held for 43.760476926s
I1013 03:44:08.202503    4835 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 03:44:08.672801    4835 ssh_runner.go:195] Run: cat /version.json
I1013 03:44:08.672956    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:08.675612    4835 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1013 03:44:08.691676    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:08.845806    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:09.668833    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:10.362180    4835 ssh_runner.go:235] Completed: cat /version.json: (1.689263145s)
I1013 03:44:10.362406    4835 ssh_runner.go:195] Run: systemctl --version
I1013 03:44:10.363012    4835 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.687367744s)
I1013 03:44:10.445421    4835 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1013 03:44:10.906590    4835 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1013 03:44:11.154796    4835 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1013 03:44:11.154895    4835 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1013 03:44:11.196568    4835 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1013 03:44:11.196595    4835 start.go:495] detecting cgroup driver to use...
I1013 03:44:11.196647    4835 detect.go:190] detected "systemd" cgroup driver on host os
I1013 03:44:11.201679    4835 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1013 03:44:12.297260    4835 ssh_runner.go:235] Completed: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml": (1.095544714s)
I1013 03:44:12.297357    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1013 03:44:12.379071    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1013 03:44:12.459219    4835 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1013 03:44:12.459302    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1013 03:44:12.546505    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1013 03:44:12.672282    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1013 03:44:12.768287    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1013 03:44:12.834391    4835 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1013 03:44:12.908528    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1013 03:44:12.979687    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1013 03:44:13.042136    4835 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1013 03:44:13.099019    4835 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1013 03:44:13.168595    4835 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1013 03:44:13.168770    4835 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1013 03:44:13.281388    4835 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1013 03:44:13.363368    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:14.092222    4835 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1013 03:44:14.775571    4835 start.go:495] detecting cgroup driver to use...
I1013 03:44:14.775635    4835 detect.go:190] detected "systemd" cgroup driver on host os
I1013 03:44:14.780914    4835 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1013 03:44:14.930784    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1013 03:44:15.011611    4835 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1013 03:44:15.236264    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1013 03:44:15.360900    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1013 03:44:15.480037    4835 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1013 03:44:15.656009    4835 ssh_runner.go:195] Run: which cri-dockerd
I1013 03:44:15.709760    4835 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1013 03:44:15.854596    4835 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1013 03:44:16.011635    4835 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1013 03:44:16.670297    4835 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1013 03:44:17.213658    4835 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1013 03:44:17.213819    4835 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1013 03:44:17.361615    4835 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1013 03:44:17.453089    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:17.920104    4835 ssh_runner.go:195] Run: sudo systemctl restart docker
I1013 03:44:25.376180    4835 ssh_runner.go:235] Completed: sudo systemctl restart docker: (7.456031716s)
I1013 03:44:25.376279    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1013 03:44:25.499004    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1013 03:44:25.598427    4835 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1013 03:44:25.744929    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1013 03:44:25.860133    4835 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1013 03:44:26.334358    4835 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1013 03:44:26.743910    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:27.139409    4835 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1013 03:44:27.248146    4835 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1013 03:44:27.343128    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:27.757320    4835 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1013 03:44:30.705839    4835 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (2.948479039s)
I1013 03:44:30.705929    4835 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1013 03:44:30.810840    4835 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1013 03:44:30.811050    4835 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1013 03:44:30.850837    4835 start.go:563] Will wait 60s for crictl version
I1013 03:44:30.850932    4835 ssh_runner.go:195] Run: which crictl
I1013 03:44:30.893397    4835 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1013 03:44:32.927234    4835 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (2.033799304s)
I1013 03:44:32.927264    4835 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1013 03:44:32.927488    4835 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1013 03:44:34.655819    4835 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (1.728287076s)
I1013 03:44:34.656873    4835 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1013 03:44:35.118022    4835 out.go:252] 🐳  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1013 03:44:35.118573    4835 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1013 03:44:35.283394    4835 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1013 03:44:35.343246    4835 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1013 03:44:35.555942    4835 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1013 03:44:35.556201    4835 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1013 03:44:35.556420    4835 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1013 03:44:35.827409    4835 docker.go:691] Got preloaded images: -- stdout --
jeiyashivani/my-gitops-app:v10
jeiyashivani/my-gitops-app:v8
jeiyashivani/my-gitops-app:v7
jeiyashivani/my-gitops-app:v6
jeiyashivani/my-gitops-app:v2
jeiyashivani/my-gitops-app:v3
jeiyashivani/my-gitops-app:<none>
jeiyashivani/my-gitops-app:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1013 03:44:35.827466    4835 docker.go:621] Images already preloaded, skipping extraction
I1013 03:44:35.827608    4835 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1013 03:44:36.166283    4835 docker.go:691] Got preloaded images: -- stdout --
jeiyashivani/my-gitops-app:v10
jeiyashivani/my-gitops-app:v8
jeiyashivani/my-gitops-app:v7
jeiyashivani/my-gitops-app:v6
jeiyashivani/my-gitops-app:v2
jeiyashivani/my-gitops-app:v3
jeiyashivani/my-gitops-app:<none>
jeiyashivani/my-gitops-app:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1013 03:44:36.166314    4835 cache_images.go:85] Images are preloaded, skipping loading
I1013 03:44:36.166333    4835 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1013 03:44:36.167170    4835 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1013 03:44:36.167319    4835 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1013 03:44:39.976252    4835 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (3.808892867s)
I1013 03:44:39.976315    4835 cni.go:84] Creating CNI manager for ""
I1013 03:44:39.976353    4835 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1013 03:44:39.976842    4835 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1013 03:44:39.976890    4835 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1013 03:44:39.990551    4835 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1013 03:44:39.991486    4835 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1013 03:44:40.171720    4835 binaries.go:44] Found k8s binaries, skipping transfer
I1013 03:44:40.171812    4835 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1013 03:44:40.278289    4835 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1013 03:44:40.628843    4835 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1013 03:44:40.855688    4835 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1013 03:44:40.892730    4835 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1013 03:44:40.904059    4835 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1013 03:44:40.939156    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:41.174398    4835 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1013 03:44:41.206747    4835 certs.go:68] Setting up /home/js/.minikube/profiles/minikube for IP: 192.168.49.2
I1013 03:44:41.206757    4835 certs.go:194] generating shared ca certs ...
I1013 03:44:41.206768    4835 certs.go:226] acquiring lock for ca certs: {Name:mkc08e73264705e19eebe99fee1586cff66aa73c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 03:44:41.208727    4835 certs.go:235] skipping valid "minikubeCA" ca cert: /home/js/.minikube/ca.key
I1013 03:44:41.209668    4835 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/js/.minikube/proxy-client-ca.key
I1013 03:44:41.209688    4835 certs.go:256] generating profile certs ...
I1013 03:44:41.210893    4835 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/js/.minikube/profiles/minikube/client.key
I1013 03:44:41.215292    4835 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/js/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1013 03:44:41.216538    4835 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/js/.minikube/profiles/minikube/proxy-client.key
I1013 03:44:41.216897    4835 certs.go:484] found cert: /home/js/.minikube/certs/ca-key.pem (1679 bytes)
I1013 03:44:41.219016    4835 certs.go:484] found cert: /home/js/.minikube/certs/ca.pem (1066 bytes)
I1013 03:44:41.219052    4835 certs.go:484] found cert: /home/js/.minikube/certs/cert.pem (1111 bytes)
I1013 03:44:41.219392    4835 certs.go:484] found cert: /home/js/.minikube/certs/key.pem (1675 bytes)
I1013 03:44:41.227875    4835 ssh_runner.go:362] scp /home/js/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1013 03:44:41.360290    4835 ssh_runner.go:362] scp /home/js/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1013 03:44:41.485650    4835 ssh_runner.go:362] scp /home/js/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1013 03:44:41.560302    4835 ssh_runner.go:362] scp /home/js/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1013 03:44:41.628283    4835 ssh_runner.go:362] scp /home/js/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1013 03:44:41.730468    4835 ssh_runner.go:362] scp /home/js/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1013 03:44:41.876965    4835 ssh_runner.go:362] scp /home/js/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1013 03:44:41.960007    4835 ssh_runner.go:362] scp /home/js/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1013 03:44:42.040836    4835 ssh_runner.go:362] scp /home/js/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1013 03:44:42.093682    4835 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1013 03:44:42.160811    4835 ssh_runner.go:195] Run: openssl version
I1013 03:44:42.190666    4835 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1013 03:44:42.262429    4835 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1013 03:44:42.281068    4835 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 12 08:04 /usr/share/ca-certificates/minikubeCA.pem
I1013 03:44:42.281111    4835 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1013 03:44:42.353499    4835 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1013 03:44:42.379562    4835 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1013 03:44:42.404845    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1013 03:44:42.477834    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1013 03:44:42.534004    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1013 03:44:42.601964    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1013 03:44:42.660021    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1013 03:44:42.759383    4835 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1013 03:44:42.865508    4835 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:2048 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 03:44:42.865658    4835 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1013 03:44:43.166045    4835 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1013 03:44:43.290522    4835 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1013 03:44:43.290531    4835 kubeadm.go:589] restartPrimaryControlPlane start ...
I1013 03:44:43.290570    4835 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1013 03:44:43.366929    4835 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1013 03:44:43.371652    4835 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I1013 03:44:43.441471    4835 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1013 03:44:43.564445    4835 kubeadm.go:626] The running cluster does not require reconfiguration: 192.168.49.2
I1013 03:44:43.564697    4835 kubeadm.go:593] duration metric: took 274.161498ms to restartPrimaryControlPlane
I1013 03:44:43.564705    4835 kubeadm.go:394] duration metric: took 699.214798ms to StartCluster
I1013 03:44:43.564719    4835 settings.go:142] acquiring lock: {Name:mke095d919e252fdec1c781c97d1c943d67c1985 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 03:44:43.564887    4835 settings.go:150] Updating kubeconfig:  /home/js/.kube/config
I1013 03:44:43.566635    4835 lock.go:35] WriteFile acquiring /home/js/.kube/config: {Name:mka319395840701282549f25dcd5543c0256e57d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 03:44:43.567744    4835 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1013 03:44:43.568034    4835 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1013 03:44:43.571427    4835 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1013 03:44:43.571488    4835 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1013 03:44:43.571503    4835 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1013 03:44:43.571507    4835 addons.go:247] addon storage-provisioner should already be in state true
I1013 03:44:43.571532    4835 host.go:66] Checking if "minikube" exists ...
I1013 03:44:43.572454    4835 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 03:44:43.579333    4835 out.go:179] 🔎  Verifying Kubernetes components...
I1013 03:44:43.594973    4835 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1013 03:44:43.595032    4835 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1013 03:44:43.595261    4835 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 03:44:43.605260    4835 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 03:44:43.771310    4835 out.go:179]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1013 03:44:43.776997    4835 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1013 03:44:43.776959    4835 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1013 03:44:43.776969    4835 addons.go:247] addon default-storageclass should already be in state true
I1013 03:44:43.777008    4835 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1013 03:44:43.776988    4835 host.go:66] Checking if "minikube" exists ...
I1013 03:44:43.777052    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:43.777214    4835 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 03:44:43.980138    4835 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1013 03:44:43.998287    4835 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1013 03:44:43.998298    4835 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1013 03:44:43.998350    4835 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 03:44:44.004486    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:44.078402    4835 api_server.go:52] waiting for apiserver process to appear ...
I1013 03:44:44.078449    4835 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1013 03:44:44.149052    4835 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/js/.minikube/machines/minikube/id_rsa Username:docker}
I1013 03:44:44.344990    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1013 03:44:44.520000    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1013 03:44:44.587145    4835 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1013 03:44:45.436822    4835 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0918095s)
W1013 03:44:45.436842    4835 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1013 03:44:45.437112    4835 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.437131    4835 retry.go:31] will retry after 154.429109ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.437199    4835 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1013 03:44:45.437247    4835 retry.go:31] will retry after 340.024718ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.474072    4835 api_server.go:72] duration metric: took 1.906301474s to wait for apiserver process to appear ...
I1013 03:44:45.474087    4835 api_server.go:88] waiting for apiserver healthz status ...
I1013 03:44:45.474103    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:45.474430    4835 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1013 03:44:45.592333    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1013 03:44:45.777640    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1013 03:44:45.808652    4835 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.808670    4835 retry.go:31] will retry after 488.538881ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1013 03:44:45.909098    4835 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.909117    4835 retry.go:31] will retry after 375.697069ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 03:44:45.974522    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:45.975374    4835 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1013 03:44:46.285179    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1013 03:44:46.298107    4835 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1013 03:44:46.474674    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:46.475354    4835 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1013 03:44:46.974385    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:50.395750    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1013 03:44:50.395775    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1013 03:44:50.395896    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:50.536892    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:50.536913    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:50.536927    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:50.605160    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:50.605182    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:50.983277    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:50.987971    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:50.987984    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:51.068461    4835 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.783255906s)
I1013 03:44:51.068502    4835 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (4.770386544s)
I1013 03:44:51.088633    4835 out.go:179] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1013 03:44:51.094112    4835 addons.go:514] duration metric: took 7.522775034s for enable addons: enabled=[storage-provisioner default-storageclass]
I1013 03:44:51.474571    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:51.482317    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:51.482338    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:51.974521    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:51.997497    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:51.997514    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:52.474358    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:52.482985    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:52.483003    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:52.974961    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:53.001908    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:53.001941    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:53.474531    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:53.523074    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:53.523103    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:53.974468    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:54.054003    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 03:44:54.054020    4835 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 03:44:54.474308    4835 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1013 03:44:54.484043    4835 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1013 03:44:54.485024    4835 api_server.go:141] control plane version: v1.34.0
I1013 03:44:54.485037    4835 api_server.go:131] duration metric: took 9.010945487s to wait for apiserver health ...
I1013 03:44:54.485046    4835 system_pods.go:43] waiting for kube-system pods to appear ...
I1013 03:44:54.506993    4835 system_pods.go:59] 7 kube-system pods found
I1013 03:44:54.507173    4835 system_pods.go:61] "coredns-66bc5c9577-dwlb4" [6e32daef-704c-4070-81ad-46c12d5f5da7] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1013 03:44:54.507182    4835 system_pods.go:61] "etcd-minikube" [eab142aa-1306-4d91-acbb-79b7df9cdedb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1013 03:44:54.507190    4835 system_pods.go:61] "kube-apiserver-minikube" [fe87e2ed-b5fa-4bdc-bec6-dcc3b706ea0d] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1013 03:44:54.507195    4835 system_pods.go:61] "kube-controller-manager-minikube" [9f9fdf29-8378-423a-81cd-54f39a0dabf8] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1013 03:44:54.507199    4835 system_pods.go:61] "kube-proxy-gj295" [6afee95b-d43c-45a2-8acf-ef5335c0ab4e] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1013 03:44:54.507205    4835 system_pods.go:61] "kube-scheduler-minikube" [e9f7c3ee-0ab3-4ed6-9040-8625aaf60415] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1013 03:44:54.507207    4835 system_pods.go:61] "storage-provisioner" [5ab2c28c-6f29-4327-9de7-54911c96f427] Running
I1013 03:44:54.507213    4835 system_pods.go:74] duration metric: took 22.161807ms to wait for pod list to return data ...
I1013 03:44:54.507226    4835 kubeadm.go:578] duration metric: took 10.939460244s to wait for: map[apiserver:true system_pods:true]
I1013 03:44:54.507245    4835 node_conditions.go:102] verifying NodePressure condition ...
I1013 03:44:54.539355    4835 node_conditions.go:122] node storage ephemeral capacity is 25623780Ki
I1013 03:44:54.539386    4835 node_conditions.go:123] node cpu capacity is 2
I1013 03:44:54.539403    4835 node_conditions.go:105] duration metric: took 32.155264ms to run NodePressure ...
I1013 03:44:54.539481    4835 start.go:241] waiting for startup goroutines ...
I1013 03:44:54.539486    4835 start.go:246] waiting for cluster config update ...
I1013 03:44:54.539496    4835 start.go:255] writing updated cluster config ...
I1013 03:44:54.539794    4835 ssh_runner.go:195] Run: rm -f paused
I1013 03:44:55.141141    4835 start.go:617] kubectl: 1.31.13, cluster: 1.34.0 (minor skew: 3)
I1013 03:44:55.144715    4835 out.go:203] 
W1013 03:44:55.148276    4835 out.go:285] ❗  /usr/bin/kubectl is version 1.31.13, which may have incompatibilities with Kubernetes 1.34.0.
I1013 03:44:55.152191    4835 out.go:179]     ▪ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1013 03:44:55.156816    4835 out.go:179] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 13 08:44:19 minikube dockerd[766]: time="2025-10-13T08:44:18.992262320Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 13 08:44:19 minikube dockerd[766]: time="2025-10-13T08:44:19.180131431Z" level=info msg="Loading containers: start."
Oct 13 08:44:24 minikube dockerd[766]: time="2025-10-13T08:44:24.298478503Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 80e9aade0881006f022ac97ae5e431f4c9095202231059ec4eedd4f795bc3058], retrying...."
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.048672729Z" level=warning msg="error locating sandbox id 405c199fbf5a658d68b77f274b6457deb34716ce2937691b25114af5255b0f74: sandbox 405c199fbf5a658d68b77f274b6457deb34716ce2937691b25114af5255b0f74 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049130583Z" level=warning msg="error locating sandbox id 4e50de59eeb749c23512d217edf5115ec8fcae151a684279bf6d134a7ea21f7a: sandbox 4e50de59eeb749c23512d217edf5115ec8fcae151a684279bf6d134a7ea21f7a not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049334877Z" level=warning msg="error locating sandbox id f657f41eaeaaab0df1fc8517231142d311ec5f884e787013a1df2f48b524ed5b: sandbox f657f41eaeaaab0df1fc8517231142d311ec5f884e787013a1df2f48b524ed5b not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049482672Z" level=warning msg="error locating sandbox id dd66e86944ff67720e4d37ae58218d71651bf0c56b696e64220b91c701e3597c: sandbox dd66e86944ff67720e4d37ae58218d71651bf0c56b696e64220b91c701e3597c not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049585763Z" level=warning msg="error locating sandbox id 66cb5a3336be6eafaa27502532e52df57b9ccb0e1bed0072438a71dfd7c375b5: sandbox 66cb5a3336be6eafaa27502532e52df57b9ccb0e1bed0072438a71dfd7c375b5 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049704341Z" level=warning msg="error locating sandbox id d3042ccd733bed033870e0334774e3bf62628a38e80f19e1a915b57457d2e338: sandbox d3042ccd733bed033870e0334774e3bf62628a38e80f19e1a915b57457d2e338 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049846830Z" level=warning msg="error locating sandbox id 37698de1a2db30269aa013c410d0c6558d79926c025094e27d142de5498c0024: sandbox 37698de1a2db30269aa013c410d0c6558d79926c025094e27d142de5498c0024 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.049971201Z" level=warning msg="error locating sandbox id e673a760ccc06a71876d4fc088314c9cf7e7617c7e06ad7bb1b284b9610f1f2c: sandbox e673a760ccc06a71876d4fc088314c9cf7e7617c7e06ad7bb1b284b9610f1f2c not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050077915Z" level=warning msg="error locating sandbox id a06355454b93d98798509eea67c4bc64909df5eb9024d8b7fcc09288da7fa3fe: sandbox a06355454b93d98798509eea67c4bc64909df5eb9024d8b7fcc09288da7fa3fe not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050176681Z" level=warning msg="error locating sandbox id b0cf80d553bd90a6cfedc62bfa375977ae1b97869c6da084cc3eb80d85ac938f: sandbox b0cf80d553bd90a6cfedc62bfa375977ae1b97869c6da084cc3eb80d85ac938f not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050276514Z" level=warning msg="error locating sandbox id ab2678cec880df5bdf96dd7e3dfbe458c96e89a235113730e0761c09df587d17: sandbox ab2678cec880df5bdf96dd7e3dfbe458c96e89a235113730e0761c09df587d17 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050377101Z" level=warning msg="error locating sandbox id dd3f80d0c187d5eb6f1431bf5ea3d5028ebe5d31ebda19b864477092e33fc81a: sandbox dd3f80d0c187d5eb6f1431bf5ea3d5028ebe5d31ebda19b864477092e33fc81a not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050517248Z" level=warning msg="error locating sandbox id df779815edd3b6a08d38bc2a8555e07b078a38c7a1df492ddceb26d83956ab5b: sandbox df779815edd3b6a08d38bc2a8555e07b078a38c7a1df492ddceb26d83956ab5b not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.050858021Z" level=warning msg="error locating sandbox id 5efce79e3cd9a2e084b0611493d278aa857002783dec6ecdb7077edc2006f511: sandbox 5efce79e3cd9a2e084b0611493d278aa857002783dec6ecdb7077edc2006f511 not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.051036408Z" level=warning msg="error locating sandbox id 9e5ab4192444fad41063790bcae2d563906ab1a5f962745fb269f663ce20596a: sandbox 9e5ab4192444fad41063790bcae2d563906ab1a5f962745fb269f663ce20596a not found"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.052716362Z" level=info msg="Loading containers: done."
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.197967602Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.199893600Z" level=info msg="Initializing buildkit"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.287906232Z" level=info msg="Completed buildkit initialization"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.343988141Z" level=info msg="Daemon has completed initialization"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.346489166Z" level=info msg="API listen on /var/run/docker.sock"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.349508743Z" level=info msg="API listen on /run/docker.sock"
Oct 13 08:44:25 minikube dockerd[766]: time="2025-10-13T08:44:25.351769964Z" level=info msg="API listen on [::]:2376"
Oct 13 08:44:25 minikube systemd[1]: Started Docker Application Container Engine.
Oct 13 08:44:27 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 13 08:44:29 minikube cri-dockerd[1078]: time="2025-10-13T08:44:29Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 13 08:44:29 minikube cri-dockerd[1078]: time="2025-10-13T08:44:29Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 13 08:44:29 minikube cri-dockerd[1078]: time="2025-10-13T08:44:29Z" level=info msg="Start docker client with request timeout 0s"
Oct 13 08:44:29 minikube cri-dockerd[1078]: time="2025-10-13T08:44:29Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Loaded network plugin cni"
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Setting cgroupDriver systemd"
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 13 08:44:30 minikube cri-dockerd[1078]: time="2025-10-13T08:44:30Z" level=info msg="Start cri-dockerd grpc backend"
Oct 13 08:44:30 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 13 08:44:42 minikube cri-dockerd[1078]: time="2025-10-13T08:44:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nodeapp-85d9688566-msrmp_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2453542be3e4d0152b1df84b65477125cbe1599b069009274cb36e0162ac250b\""
Oct 13 08:44:42 minikube cri-dockerd[1078]: time="2025-10-13T08:44:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nodeapp-85d9688566-xq4mx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dd7d8ce1b29b9652ff8c4fd7175c7b2961b72ca98519e169b2c2df834d48fb66\""
Oct 13 08:44:42 minikube cri-dockerd[1078]: time="2025-10-13T08:44:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nodeapp-85d9688566-4t689_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"507dcee389f8f981dbb4e0f5c07ee74fb3f925fd93adc54e2588ffdb75d09025\""
Oct 13 08:44:42 minikube cri-dockerd[1078]: time="2025-10-13T08:44:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-dwlb4_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60d8661e4164f152b7978bc08977c75ed67c8fdd349ef1de1f964fc0e8030243\""
Oct 13 08:44:42 minikube cri-dockerd[1078]: time="2025-10-13T08:44:42Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-dwlb4_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"079ca39eba522dec01517f7636187e74e73b817973038f73198d3bd98881bb90\""
Oct 13 08:44:44 minikube cri-dockerd[1078]: time="2025-10-13T08:44:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bb8ea08d460110df8c6c4f4315d476a711efc89a9aa4c01129aaf2a6026085b/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Oct 13 08:44:44 minikube cri-dockerd[1078]: time="2025-10-13T08:44:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a07fa56aa0fd1fb49aa390fdaf8d0166321aff4985e355bfe064273dccee877/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Oct 13 08:44:44 minikube cri-dockerd[1078]: time="2025-10-13T08:44:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4cc089e88e80b553c9f5e8d98e4f4407478ab5cfcb2b275d6930990d6f20fd17/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Oct 13 08:44:44 minikube cri-dockerd[1078]: time="2025-10-13T08:44:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f10737398bad5ba3d1578f10df47aa07d6b51fc3a5b4dc677e5eb2c9d4edaf41/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Oct 13 08:44:45 minikube cri-dockerd[1078]: time="2025-10-13T08:44:45Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-dwlb4_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60d8661e4164f152b7978bc08977c75ed67c8fdd349ef1de1f964fc0e8030243\""
Oct 13 08:44:50 minikube cri-dockerd[1078]: time="2025-10-13T08:44:50Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 13 08:44:52 minikube cri-dockerd[1078]: time="2025-10-13T08:44:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/18a57cc7d1dbea6bfd791cb064d60daecaf00b0e8fa532364ef25c0c6e4668cf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 13 08:44:52 minikube cri-dockerd[1078]: time="2025-10-13T08:44:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c825129e35d4c0403773157f0ec9212637840876af59146eb435e06ba05c57de/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 13 08:44:52 minikube cri-dockerd[1078]: time="2025-10-13T08:44:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/441e9d0608e2dbdec9f9363f06718dc168e60b2caa99c290346cb1ecf1fb0415/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Oct 13 08:44:53 minikube cri-dockerd[1078]: time="2025-10-13T08:44:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3e074ede66409f1f3da66a076849d355f9e5f692f9bd531d8e912ce7127cae4f/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Oct 13 08:44:53 minikube cri-dockerd[1078]: time="2025-10-13T08:44:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d6f08c3cc497182fbf0025655c92d323435928459055b3eb02626125dc929c41/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Oct 13 08:44:54 minikube cri-dockerd[1078]: time="2025-10-13T08:44:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/12cbfc8655ca92cda3176a78910984dc64f0ae3febb5b8e2bd776b31cd82dabd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 13 08:44:56 minikube cri-dockerd[1078]: time="2025-10-13T08:44:56Z" level=info msg="Stop pulling image jeiyashivani/my-gitops-app:v10: Status: Image is up to date for jeiyashivani/my-gitops-app:v10"
Oct 13 08:44:59 minikube cri-dockerd[1078]: time="2025-10-13T08:44:59Z" level=info msg="Stop pulling image jeiyashivani/my-gitops-app:v10: Status: Image is up to date for jeiyashivani/my-gitops-app:v10"
Oct 13 08:45:09 minikube cri-dockerd[1078]: time="2025-10-13T08:45:09Z" level=info msg="Stop pulling image jeiyashivani/my-gitops-app:v10: Status: Image is up to date for jeiyashivani/my-gitops-app:v10"
Oct 13 08:45:25 minikube dockerd[766]: time="2025-10-13T08:45:25.143896893Z" level=info msg="ignoring event" container=0d4616cf8c4a75c3430ca088fc2645b6d1541cf621cde41536e33176c53d3c07 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
32f93fae00aaf       6e38f40d628db                                                                                        55 seconds ago       Running             storage-provisioner       6                   441e9d0608e2d       storage-provisioner
3d9c7a8c782a8       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   About a minute ago   Running             node-rest-api             1                   12cbfc8655ca9       nodeapp-85d9688566-xq4mx
1c1410722e374       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   About a minute ago   Running             node-rest-api             1                   c825129e35d4c       nodeapp-85d9688566-4t689
48b820b240f98       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   About a minute ago   Running             node-rest-api             1                   18a57cc7d1dbe       nodeapp-85d9688566-msrmp
bf4e4bd375b89       df0860106674d                                                                                        About a minute ago   Running             kube-proxy                2                   d6f08c3cc4971       kube-proxy-gj295
2bbd77744f3cf       52546a367cc9e                                                                                        About a minute ago   Running             coredns                   2                   3e074ede66409       coredns-66bc5c9577-dwlb4
0d4616cf8c4a7       6e38f40d628db                                                                                        About a minute ago   Exited              storage-provisioner       5                   441e9d0608e2d       storage-provisioner
4f1c9f1cf08d2       a0af72f2ec6d6                                                                                        About a minute ago   Running             kube-controller-manager   3                   f10737398bad5       kube-controller-manager-minikube
fb9b2f56572c3       46169d968e920                                                                                        About a minute ago   Running             kube-scheduler            2                   4cc089e88e80b       kube-scheduler-minikube
8a5520cb7f4db       5f1f5298c888d                                                                                        About a minute ago   Running             etcd                      2                   5a07fa56aa0fd       etcd-minikube
60d23a9198bd2       90550c43ad2bc                                                                                        About a minute ago   Running             kube-apiserver            2                   3bb8ea08d4601       kube-apiserver-minikube
5c925a1820f93       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   2 hours ago          Exited              node-rest-api             0                   2453542be3e4d       nodeapp-85d9688566-msrmp
6bee3c13c34dd       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   2 hours ago          Exited              node-rest-api             0                   dd7d8ce1b29b9       nodeapp-85d9688566-xq4mx
1d74fa9e539ab       jeiyashivani/my-gitops-app@sha256:992bac6071e2649ac25e477739e743daacaf6a462f596b7dfb4682a739f1e9d9   2 hours ago          Exited              node-rest-api             0                   507dcee389f8f       nodeapp-85d9688566-4t689
dce54201ff3ec       a0af72f2ec6d6                                                                                        4 hours ago          Exited              kube-controller-manager   2                   69f2b8cfe70c9       kube-controller-manager-minikube
b3117f1b66fc2       df0860106674d                                                                                        4 hours ago          Exited              kube-proxy                1                   a83851315c4c9       kube-proxy-gj295
0e8b6b1d8ddec       52546a367cc9e                                                                                        4 hours ago          Exited              coredns                   1                   60d8661e4164f       coredns-66bc5c9577-dwlb4
e23019ca55e8c       46169d968e920                                                                                        4 hours ago          Exited              kube-scheduler            1                   617f61ac187f9       kube-scheduler-minikube
6ce5646b69d7e       5f1f5298c888d                                                                                        4 hours ago          Exited              etcd                      1                   23015e7db5c58       etcd-minikube
6346d666b3fe5       90550c43ad2bc                                                                                        4 hours ago          Exited              kube-apiserver            1                   56004a7299f9d       kube-apiserver-minikube


==> coredns [0e8b6b1d8dde] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:46742 - 38259 "HINFO IN 9026204910415920942.3915589465461707423. udp 57 false 512" NXDOMAIN qr,rd,ra 132 2.231686003s
[INFO] 127.0.0.1:57190 - 5225 "HINFO IN 9026204910415920942.3915589465461707423. udp 57 false 512" NXDOMAIN qr,aa,rd,ra 132 0.00061614s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.148535335s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.295717972s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.410769918s
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.52594587s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.381818699s
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.021835455s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.095694459s


==> coredns [2bbd77744f3c] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:44384 - 56364 "HINFO IN 4955187512875126128.4815123481898572548. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.146844779s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_12T12_12_46_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Oct 2025 17:12:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 13 Oct 2025 08:46:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 13 Oct 2025 08:44:50 +0000   Sun, 12 Oct 2025 17:12:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 13 Oct 2025 08:44:50 +0000   Sun, 12 Oct 2025 17:12:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 13 Oct 2025 08:44:50 +0000   Sun, 12 Oct 2025 17:12:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 13 Oct 2025 08:44:50 +0000   Mon, 13 Oct 2025 06:59:25 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  25623780Ki
  hugepages-2Mi:      0
  memory:             3912272Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  25623780Ki
  hugepages-2Mi:      0
  memory:             3912272Ki
  pods:               110
System Info:
  Machine ID:                 819c7260601b4d8f9694f9ac7149dacc
  System UUID:                5fb3559b-6641-4c0f-a190-f9555a6d1527
  Boot ID:                    c1d280c7-1cd7-45ad-865d-0f11832325c4
  Kernel Version:             6.14.0-29-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nodeapp-85d9688566-4t689            0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m
  default                     nodeapp-85d9688566-msrmp            0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m
  default                     nodeapp-85d9688566-xq4mx            0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m
  kube-system                 coredns-66bc5c9577-dwlb4            100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     15h
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         15h
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         15h
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         15h
  kube-system                 kube-proxy-gj295                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15h
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         15h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         15h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 99s                  kube-proxy       
  Normal   NodeNotReady             107m (x3 over 171m)  kubelet          Node minikube status is now: NodeNotReady
  Normal   NodeReady                107m (x2 over 171m)  kubelet          Node minikube status is now: NodeReady
  Normal   Starting                 113s                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  113s (x8 over 113s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    113s (x8 over 113s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     113s (x7 over 113s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  113s                 kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 105s                 kubelet          Node minikube has been rebooted, boot id: c1d280c7-1cd7-45ad-865d-0f11832325c4
  Normal   RegisteredNode           100s                 node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct13 08:38] APIC calibration not consistent with PM-Timer: 130ms instead of 100ms
[  +0.004998] TSC synchronization [CPU#0 -> CPU#1]:
[  +0.000000] Measured 96101 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.032643] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended configuration space under this bridge
[  +0.305438] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +7.663037] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +0.011011] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +1.578966] systemd[1]: Invalid DMI field header.
[ +14.123338] kauditd_printk_skb: 121 callbacks suppressed
[  +2.845313] vmwgfx 0000:00:02.0: [drm] *ERROR* vmwgfx seems to be running on an unsupported hypervisor.
[  +0.000009] vmwgfx 0000:00:02.0: [drm] *ERROR* This configuration is likely broken.
[  +0.000004] vmwgfx 0000:00:02.0: [drm] *ERROR* Please switch to a supported graphics device to avoid problems.
[  +2.856768] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +1.176531] kauditd_printk_skb: 13 callbacks suppressed
[  +0.290216] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[ +11.718293] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Oct13 08:39] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[ +19.235240] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +5.184355] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[ +17.344445] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Oct13 08:40] vboxsf: Unknown parameter 'tag'
[  +3.479181] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[ +40.699192] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[  +2.048204] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Oct13 08:42] /dev/sr0: Can't open blockdev
[Oct13 08:43] workqueue: blk_mq_requeue_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[ +18.828733] workqueue: blk_mq_requeue_work hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +0.291726] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[  +0.941383] workqueue: blk_mq_requeue_work hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Oct13 08:44] hrtimer: interrupt took 17905085 ns
[Oct13 08:45] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND


==> etcd [6ce5646b69d7] <==
{"level":"warn","ts":"2025-10-13T06:52:05.039992Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"6.415µs","request":"header:<ID:8128040597016876740 > lease_revoke:<id:70cc99dbd92507b4>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040008Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"4.076µs","request":"header:<ID:8128040597016876741 > lease_revoke:<id:70cc99dbd9250925>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040026Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"4.018µs","request":"header:<ID:8128040597016876742 > lease_revoke:<id:70cc99dbd9250afe>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040041Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"3.872µs","request":"header:<ID:8128040597016876743 > lease_revoke:<id:70cc99dbd9250d10>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040056Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"4.011µs","request":"header:<ID:8128040597016876744 > lease_revoke:<id:70cc99dbd9250ef8>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040072Z","caller":"txn/util.go:46","msg":"failed to apply request","took":"3.934µs","request":"header:<ID:8128040597016876745 > lease_revoke:<id:70cc99dbd9251188>","response":"size:29","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040823Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd925165d","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040962Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd924fd86","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040987Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd92516a1","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.040998Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd924ffb5","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041011Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd925013d","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041018Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9250316","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041028Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd92504c5","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041035Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd925064f","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041046Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd92507b4","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041053Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9250925","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041063Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9250afe","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041071Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9250d10","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041080Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9250ef8","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.041088Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc99dbd9251188","error":"lease not found"}
{"level":"warn","ts":"2025-10-13T06:52:05.246090Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59924","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:59924: read: connection reset by peer"}
{"level":"warn","ts":"2025-10-13T06:52:05.284027Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:59900","server-name":"","error":"read tcp 127.0.0.1:2379->127.0.0.1:59900: read: connection reset by peer"}
{"level":"warn","ts":"2025-10-13T06:52:14.968869Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"152.366972ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-10-13T06:52:15.058526Z","caller":"traceutil/trace.go:172","msg":"trace[1715318812] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:10406; }","duration":"294.814043ms","start":"2025-10-13T06:52:14.763685Z","end":"2025-10-13T06:52:15.058499Z","steps":["trace[1715318812] 'agreement among raft nodes before linearized reading'  (duration: 20.299887ms)","trace[1715318812] 'range keys from in-memory index tree'  (duration: 76.599003ms)","trace[1715318812] 'range keys from bolt db'  (duration: 42.036119ms)"],"step_count":3}
{"level":"info","ts":"2025-10-13T06:52:15.484457Z","caller":"traceutil/trace.go:172","msg":"trace[722394396] transaction","detail":"{read_only:false; response_revision:10407; number_of_response:1; }","duration":"136.6074ms","start":"2025-10-13T06:52:15.347815Z","end":"2025-10-13T06:52:15.484423Z","steps":["trace[722394396] 'process raft request'  (duration: 87.111796ms)","trace[722394396] 'compare'  (duration: 14.694637ms)","trace[722394396] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/events/default/nodeapp-8d9d757cc-ntlhc.186df41e2e70c42e; req_size:731; } (duration: 34.395999ms)"],"step_count":3}
{"level":"info","ts":"2025-10-13T06:55:47.232184Z","caller":"traceutil/trace.go:172","msg":"trace[2006547561] transaction","detail":"{read_only:false; response_revision:10793; number_of_response:1; }","duration":"736.203901ms","start":"2025-10-13T06:55:46.495932Z","end":"2025-10-13T06:55:47.232136Z","steps":["trace[2006547561] 'process raft request'  (duration: 644.947168ms)","trace[2006547561] 'compare'  (duration: 87.241866ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-13T06:55:47.232412Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-13T06:55:46.495892Z","time spent":"736.434914ms","remote":"127.0.0.1:49488","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:10782 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128040597016878358 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2025-10-13T06:56:16.608481Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"114.964888ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-10-13T06:56:16.612685Z","caller":"traceutil/trace.go:172","msg":"trace[1864464102] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:10845; }","duration":"119.16692ms","start":"2025-10-13T06:56:16.493481Z","end":"2025-10-13T06:56:16.612648Z","steps":["trace[1864464102] 'range keys from in-memory index tree'  (duration: 113.520476ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T06:57:34.488739Z","caller":"traceutil/trace.go:172","msg":"trace[1329241757] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:10922; }","duration":"103.427065ms","start":"2025-10-13T06:57:34.385287Z","end":"2025-10-13T06:57:34.488714Z","steps":["trace[1329241757] 'agreement among raft nodes before linearized reading'  (duration: 103.273536ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T06:57:34.493167Z","caller":"traceutil/trace.go:172","msg":"trace[1342521836] transaction","detail":"{read_only:false; response_revision:10923; number_of_response:1; }","duration":"122.450473ms","start":"2025-10-13T06:57:34.370698Z","end":"2025-10-13T06:57:34.493148Z","steps":["trace[1342521836] 'process raft request'  (duration: 122.313235ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T06:58:04.857899Z","caller":"traceutil/trace.go:172","msg":"trace[1680883091] transaction","detail":"{read_only:false; response_revision:10952; number_of_response:1; }","duration":"137.108662ms","start":"2025-10-13T06:58:04.720757Z","end":"2025-10-13T06:58:04.857866Z","steps":["trace[1680883091] 'process raft request'  (duration: 101.259464ms)","trace[1680883091] 'compare'  (duration: 33.314893ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-13T06:59:15.624855Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"876.058542ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040597016879265 > lease_revoke:<id:70cc99dbd9252090>","response":"size:29"}
{"level":"info","ts":"2025-10-13T07:02:05.342782Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":10893}
{"level":"info","ts":"2025-10-13T07:02:05.483168Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":10893,"took":"133.582949ms","hash":93600171,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1626112,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-10-13T07:02:05.484756Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":93600171,"revision":10893,"compact-revision":9736}
{"level":"warn","ts":"2025-10-13T07:04:46.409687Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"116.474796ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-10-13T07:04:46.409873Z","caller":"traceutil/trace.go:172","msg":"trace[1134620053] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:11275; }","duration":"116.677208ms","start":"2025-10-13T07:04:46.293185Z","end":"2025-10-13T07:04:46.409862Z","steps":["trace[1134620053] 'agreement among raft nodes before linearized reading'  (duration: 116.327886ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-13T07:04:46.411216Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"114.785971ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-13T07:04:46.411453Z","caller":"traceutil/trace.go:172","msg":"trace[1552559378] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:11275; }","duration":"118.670059ms","start":"2025-10-13T07:04:46.292766Z","end":"2025-10-13T07:04:46.411436Z","steps":["trace[1552559378] 'range keys from in-memory index tree'  (duration: 114.505497ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-13T07:04:46.417697Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"123.710106ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingressclasses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-13T07:04:46.417867Z","caller":"traceutil/trace.go:172","msg":"trace[753924187] range","detail":"{range_begin:/registry/ingressclasses; range_end:; response_count:0; response_revision:11275; }","duration":"123.881882ms","start":"2025-10-13T07:04:46.293970Z","end":"2025-10-13T07:04:46.417852Z","steps":["trace[753924187] 'agreement among raft nodes before linearized reading'  (duration: 72.360425ms)","trace[753924187] 'range keys from in-memory index tree'  (duration: 51.320266ms)"],"step_count":2}
{"level":"info","ts":"2025-10-13T07:04:46.420881Z","caller":"traceutil/trace.go:172","msg":"trace[652751024] transaction","detail":"{read_only:false; response_revision:11276; number_of_response:1; }","duration":"126.996458ms","start":"2025-10-13T07:04:46.293866Z","end":"2025-10-13T07:04:46.420863Z","steps":["trace[652751024] 'process raft request'  (duration: 119.018401ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T07:06:15.963893Z","caller":"traceutil/trace.go:172","msg":"trace[1138170296] transaction","detail":"{read_only:false; response_revision:11511; number_of_response:1; }","duration":"108.056651ms","start":"2025-10-13T07:06:15.855809Z","end":"2025-10-13T07:06:15.963866Z","steps":["trace[1138170296] 'process raft request'  (duration: 91.685212ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T07:07:05.391754Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":11130}
{"level":"info","ts":"2025-10-13T07:07:05.408556Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":11130,"took":"16.573536ms","hash":736190591,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":2342912,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-10-13T07:07:05.408623Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":736190591,"revision":11130,"compact-revision":10893}
{"level":"info","ts":"2025-10-13T07:12:05.415292Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":11581}
{"level":"info","ts":"2025-10-13T07:12:05.425634Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":11581,"took":"10.089935ms","hash":3645310035,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":2625536,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-10-13T07:12:05.425701Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3645310035,"revision":11581,"compact-revision":11130}
{"level":"info","ts":"2025-10-13T07:15:30.127619Z","caller":"traceutil/trace.go:172","msg":"trace[1913675717] transaction","detail":"{read_only:false; response_revision:12231; number_of_response:1; }","duration":"114.435843ms","start":"2025-10-13T07:15:30.013168Z","end":"2025-10-13T07:15:30.127604Z","steps":["trace[1913675717] 'process raft request'  (duration: 114.335572ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T07:17:05.457821Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":11962}
{"level":"info","ts":"2025-10-13T07:17:05.487845Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":11962,"took":"29.778331ms","hash":1412132043,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":2351104,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-10-13T07:17:05.487946Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1412132043,"revision":11962,"compact-revision":11581}
{"level":"info","ts":"2025-10-13T07:22:05.490647Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12306}
{"level":"info","ts":"2025-10-13T07:22:05.512862Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12306,"took":"21.749369ms","hash":63808460,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":2031616,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-10-13T07:22:05.513066Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":63808460,"revision":12306,"compact-revision":11962}
{"level":"info","ts":"2025-10-13T07:27:05.520383Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12538}
{"level":"info","ts":"2025-10-13T07:27:05.525239Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12538,"took":"3.833081ms","hash":714409319,"current-db-size-bytes":4370432,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1662976,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-10-13T07:27:05.525313Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":714409319,"revision":12538,"compact-revision":12306}


==> etcd [8a5520cb7f4d] <==
{"level":"warn","ts":"2025-10-13T08:44:48.960140Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36382","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:48.974385Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36404","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:48.983908Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36426","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:48.997295Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36454","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.005578Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36464","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.017227Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36486","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.025774Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36506","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.033058Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36516","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.058361Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.069825Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36572","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.080622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36594","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.088429Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36608","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.102296Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36626","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.112383Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.160182Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36666","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.169463Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36684","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.183375Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36712","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.201156Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36734","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.211008Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36752","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.229797Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36770","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.244064Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36788","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.254155Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36810","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.267703Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36830","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.276429Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36846","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.284534Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36862","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.291768Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36886","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.300532Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36906","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.309271Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36912","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.319121Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36930","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.329204Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36948","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.342494Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36962","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.357310Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.369120Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:36998","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.378482Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37014","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.390189Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37028","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.399196Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37042","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.420337Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37062","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.430733Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37078","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.442283Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37092","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.461537Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37118","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.469746Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.478637Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37148","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.488952Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37156","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.497381Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.505679Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37212","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.519652Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37226","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.529622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37232","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.540227Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.551746Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37260","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.559420Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37274","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.571601Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37292","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.593648Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37306","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.603073Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37324","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.613551Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37336","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-10-13T08:44:49.695166Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:37360","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-10-13T08:44:52.211796Z","caller":"traceutil/trace.go:172","msg":"trace[1188527784] transaction","detail":"{read_only:false; response_revision:12879; number_of_response:1; }","duration":"106.001414ms","start":"2025-10-13T08:44:52.105780Z","end":"2025-10-13T08:44:52.211781Z","steps":["trace[1188527784] 'process raft request'  (duration: 105.729709ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T08:44:52.556860Z","caller":"traceutil/trace.go:172","msg":"trace[5856156] linearizableReadLoop","detail":"{readStateIndex:15335; appliedIndex:15335; }","duration":"105.654092ms","start":"2025-10-13T08:44:52.451192Z","end":"2025-10-13T08:44:52.556846Z","steps":["trace[5856156] 'read index received'  (duration: 105.651217ms)","trace[5856156] 'applied index is now lower than readState.Index'  (duration: 2.381µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-13T08:44:52.569586Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"118.353078ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:node\" limit:1 ","response":"range_response_count:1 size:1605"}
{"level":"info","ts":"2025-10-13T08:44:52.570360Z","caller":"traceutil/trace.go:172","msg":"trace[1859301573] range","detail":"{range_begin:/registry/clusterroles/system:node; range_end:; response_count:1; response_revision:12884; }","duration":"119.154597ms","start":"2025-10-13T08:44:52.451188Z","end":"2025-10-13T08:44:52.570343Z","steps":["trace[1859301573] 'agreement among raft nodes before linearized reading'  (duration: 105.767163ms)"],"step_count":1}
{"level":"info","ts":"2025-10-13T08:46:26.666358Z","caller":"traceutil/trace.go:172","msg":"trace[1361053830] transaction","detail":"{read_only:false; response_revision:13036; number_of_response:1; }","duration":"170.96189ms","start":"2025-10-13T08:46:26.495379Z","end":"2025-10-13T08:46:26.666341Z","steps":[],"step_count":0}


==> kernel <==
 08:46:35 up 8 min,  0 users,  load average: 3.45, 3.19, 1.63
Linux minikube 6.14.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Aug 14 16:52:50 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [60d23a9198bd] <==
I1013 08:44:50.353339       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1013 08:44:50.353356       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1013 08:44:50.354251       1 controller.go:119] Starting legacy_token_tracking_controller
I1013 08:44:50.354371       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1013 08:44:50.354436       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1013 08:44:50.355263       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1013 08:44:50.358689       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1013 08:44:50.358937       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1013 08:44:50.359021       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1013 08:44:50.359420       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1013 08:44:50.359642       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1013 08:44:50.363667       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1013 08:44:50.363736       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1013 08:44:50.364887       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1013 08:44:50.370840       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1013 08:44:50.370899       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1013 08:44:50.398700       1 repairip.go:210] Starting ipallocator-repair-controller
I1013 08:44:50.398769       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1013 08:44:50.400027       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1013 08:44:50.400076       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1013 08:44:50.401627       1 controller.go:142] Starting OpenAPI controller
I1013 08:44:50.401698       1 controller.go:90] Starting OpenAPI V3 controller
I1013 08:44:50.401716       1 naming_controller.go:299] Starting NamingConditionController
I1013 08:44:50.401730       1 establishing_controller.go:81] Starting EstablishingController
I1013 08:44:50.401742       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1013 08:44:50.401869       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1013 08:44:50.401880       1 crd_finalizer.go:269] Starting CRDFinalizer
I1013 08:44:50.519100       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1013 08:44:50.519176       1 policy_source.go:240] refreshing policies
I1013 08:44:50.534100       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1013 08:44:50.535718       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1013 08:44:50.545555       1 cache.go:39] Caches are synced for LocalAvailability controller
I1013 08:44:50.545857       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1013 08:44:50.545897       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1013 08:44:50.546210       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1013 08:44:50.546314       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1013 08:44:50.546346       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1013 08:44:50.573056       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1013 08:44:50.575062       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1013 08:44:50.586232       1 aggregator.go:171] initial CRD sync complete...
I1013 08:44:50.586306       1 autoregister_controller.go:144] Starting autoregister controller
I1013 08:44:50.586312       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1013 08:44:50.590513       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1013 08:44:50.590714       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1013 08:44:50.590901       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1013 08:44:50.593516       1 cache.go:39] Caches are synced for autoregister controller
I1013 08:44:50.594657       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1013 08:44:50.596651       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
E1013 08:44:50.676635       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1013 08:44:51.335626       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1013 08:44:51.354442       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E1013 08:44:55.873133       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 08:44:55.873121725" prevR="0.86391315ss" incrR="184467440737.09549194ss" currentR="0.86388893ss"
I1013 08:44:56.014084       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1013 08:44:56.225938       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1013 08:44:56.289965       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1013 08:44:56.354742       1 controller.go:667] quota admission added evaluator for: endpoints
I1013 08:44:56.411864       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1013 08:45:50.238863       1 stats.go:136] "Error getting keys" err="empty key: \"\""
E1013 08:45:59.367044       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 08:45:59.367002684" prevR="1.76938638ss" incrR="184467440737.09549818ss" currentR="1.76936840ss"
I1013 08:46:09.673632       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [6346d666b3fe] <==
I1013 06:57:06.815952       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 06:57:56.151238       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 06:59:14.481479       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 06:59:14.647397       1 stats.go:136] "Error getting keys" err="empty key: \"\""
E1013 06:59:45.595114       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 06:59:45.595102120" prevR="1325.29105221ss" incrR="184467440737.09544605ss" currentR="1325.29098210ss"
I1013 07:00:37.199607       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:00:44.294147       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:01:53.601008       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:02:00.383176       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:02:06.309051       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1013 07:02:56.733969       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:03:22.212331       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:04:12.117691       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:04:50.368490       1 stats.go:136] "Error getting keys" err="empty key: \"\""
E1013 07:04:57.448847       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 07:04:57.448831154" prevR="1332.53385024ss" incrR="184467440737.09549475ss" currentR="1332.53382883ss"
E1013 07:04:57.458662       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 07:04:57.458650101" prevR="1332.54367174ss" incrR="184467440737.09549220ss" currentR="1332.54364778ss"
I1013 07:05:14.100565       1 stats.go:136] "Error getting keys" err="empty key: \"\""
E1013 07:05:48.547780       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="system" when="2025-10-13 07:05:48.547748115" prevR="79.27160900ss" incrR="184467440737.09544960ss" currentR="79.27154244ss"
I1013 07:05:51.784548       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:06:23.068715       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:07:16.622841       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:07:37.630272       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:08:26.316977       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:09:00.764682       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:09:53.661742       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:10:11.510792       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:11:12.106731       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:11:15.318362       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:12:06.312295       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1013 07:12:26.951639       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:12:27.905036       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:13:48.865247       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:13:54.667144       1 stats.go:136] "Error getting keys" err="empty key: \"\""
E1013 07:14:17.580932       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-10-13 07:14:17.580898344" prevR="1341.15774031ss" incrR="184467440737.09545708ss" currentR="1341.15768123ss"
I1013 07:14:56.635900       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:15:15.198174       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:16:05.410703       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:16:21.181622       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:17:21.428563       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:17:31.665006       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:18:33.304724       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:18:56.470126       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:19:40.878398       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:20:07.920265       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:20:41.499787       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:21:09.810157       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:21:44.469947       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:22:06.312901       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1013 07:22:34.738550       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:22:47.330729       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:23:41.753565       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:24:00.312752       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:24:56.876845       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:25:23.353200       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:26:03.298192       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:26:27.358619       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:27:11.574493       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:27:37.360611       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:28:13.419552       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1013 07:28:38.957989       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [4f1c9f1cf08d] <==
I1013 08:44:55.778046       1 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
I1013 08:44:55.778066       1 shared_informer.go:349] "Waiting for caches to sync" controller="service account"
I1013 08:44:55.807268       1 controllermanager.go:781] "Started controller" controller="deployment-controller"
I1013 08:44:55.807330       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I1013 08:44:55.810530       1 shared_informer.go:349] "Waiting for caches to sync" controller="deployment"
I1013 08:44:55.816036       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1013 08:44:55.856717       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1013 08:44:55.877094       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1013 08:44:55.879456       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1013 08:44:55.879450       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1013 08:44:55.879459       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1013 08:44:55.879470       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1013 08:44:55.888266       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1013 08:44:55.891070       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1013 08:44:55.893620       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1013 08:44:55.894764       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1013 08:44:55.904701       1 shared_informer.go:356] "Caches are synced" controller="node"
I1013 08:44:55.905304       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1013 08:44:55.906226       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1013 08:44:55.906939       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1013 08:44:55.907721       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1013 08:44:55.908179       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1013 08:44:55.908436       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1013 08:44:55.908779       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1013 08:44:55.910021       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1013 08:44:55.910390       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1013 08:44:55.911370       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1013 08:44:55.911593       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1013 08:44:55.913112       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1013 08:44:55.913166       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1013 08:44:55.913195       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1013 08:44:55.917225       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1013 08:44:55.919342       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1013 08:44:55.919899       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1013 08:44:55.920154       1 shared_informer.go:356] "Caches are synced" controller="job"
I1013 08:44:55.920170       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1013 08:44:55.924489       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1013 08:44:55.924500       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1013 08:44:55.926440       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1013 08:44:55.930094       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1013 08:44:55.931652       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1013 08:44:55.935223       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1013 08:44:55.939459       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1013 08:44:55.939512       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1013 08:44:55.948904       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1013 08:44:55.955075       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1013 08:44:55.955523       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1013 08:44:55.958752       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1013 08:44:55.959660       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1013 08:44:55.960006       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1013 08:44:55.962682       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1013 08:44:55.965440       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1013 08:44:55.971184       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1013 08:44:55.979042       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1013 08:44:55.983448       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1013 08:44:55.983465       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1013 08:44:55.983470       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1013 08:44:55.983532       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1013 08:44:55.987842       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1013 08:44:55.991021       1 shared_informer.go:356] "Caches are synced" controller="cronjob"


==> kube-controller-manager [dce54201ff3e] <==
E1013 05:54:45.715913       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceclaims?allowWatchBookmarks=true&resourceVersion=10011&timeout=7m21s&timeoutSeconds=441&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
I1013 05:54:45.716066       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716107       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Ingress" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716139       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716169       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Job" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716283       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IPAddress" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E1013 05:54:45.716348       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=10048&timeout=9m47s&timeoutSeconds=587&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
I1013 05:54:45.716389       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716422       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716458       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ControllerRevision" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716488       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716517       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716544       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716574       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.FlowSchema" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716600       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v2.HorizontalPodAutoscaler" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716625       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716707       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.LimitRange" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E1013 05:54:45.716767       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/leases?allowWatchBookmarks=true&resourceVersion=10147&timeout=6m32s&timeoutSeconds=392&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Lease"
I1013 05:54:45.716805       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716836       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716865       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.716983       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:45.717076       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Role" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.098853       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.098922       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRole" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.113906       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingWebhookConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.113990       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRoleBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.114044       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceQuota" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.114091       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.114134       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicy" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126042       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodTemplate" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126114       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E1013 05:54:46.126209       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattributesclasses?allowWatchBookmarks=true&resourceVersion=10098&timeout=6m54s&timeoutSeconds=414&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttributesClass"
I1013 05:54:46.126266       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126325       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.MutatingWebhookConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126443       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Secret" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126479       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126505       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CronJob" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126529       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RoleBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126638       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicyBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126675       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126729       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceAccount" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E1013 05:54:46.126788       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/apps/v1/daemonsets?allowWatchBookmarks=true&resourceVersion=10114&timeout=9m8s&timeoutSeconds=548&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DaemonSet"
I1013 05:54:46.126829       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126859       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Deployment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126893       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IngressClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126923       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126950       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.126976       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.NetworkPolicy" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.127030       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityLevelConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I1013 05:54:46.127146       1 reflector.go:568] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E1013 05:54:49.662978       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I1013 05:54:49.853777       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1013 05:55:05.026860       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1013 06:52:03.817696       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="Get \"https://192.168.49.2:8443/api\": net/http: TLS handshake timeout"
E1013 06:52:03.817979       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: Get \"https://192.168.49.2:8443/api\": net/http: TLS handshake timeout" logger="UnhandledError"
E1013 06:52:03.845008       1 node_lifecycle_controller.go:967] "Error updating node" err="Put \"https://192.168.49.2:8443/api/v1/nodes/minikube/status\": net/http: TLS handshake timeout" logger="node-lifecycle-controller" node="minikube"
E1013 06:59:15.767959       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I1013 06:59:15.925922       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1013 06:59:30.939710       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"


==> kube-proxy [b3117f1b66fc] <==
I1013 04:34:50.623579       1 server_linux.go:53] "Using iptables proxy"
I1013 04:34:51.013459       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1013 04:34:51.121131       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1013 04:34:51.121765       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1013 04:34:51.123063       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1013 04:34:51.309518       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1013 04:34:51.309566       1 server_linux.go:132] "Using iptables Proxier"
I1013 04:34:51.417408       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1013 04:34:51.440430       1 server.go:527] "Version info" version="v1.34.0"
I1013 04:34:51.440793       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 04:34:51.450466       1 config.go:200] "Starting service config controller"
I1013 04:34:51.456523       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1013 04:34:51.457747       1 config.go:309] "Starting node config controller"
I1013 04:34:51.457759       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1013 04:34:51.457764       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1013 04:34:51.452151       1 config.go:106] "Starting endpoint slice config controller"
I1013 04:34:51.458035       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1013 04:34:51.452175       1 config.go:403] "Starting serviceCIDR config controller"
I1013 04:34:51.458048       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1013 04:34:51.561326       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1013 04:34:51.564792       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1013 04:34:51.564817       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
E1013 06:52:04.776675       1 reflector.go:205] "Failed to watch" err="Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?allowWatchBookmarks=true&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=10137&timeout=9m9s&timeoutSeconds=549&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E1013 06:52:04.781844       1 reflector.go:205] "Failed to watch" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube&resourceVersion=10252&timeout=8m13s&timeoutSeconds=493&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"


==> kube-proxy [bf4e4bd375b8] <==
I1013 08:44:55.570434       1 server_linux.go:53] "Using iptables proxy"
I1013 08:44:55.691595       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1013 08:44:55.794271       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1013 08:44:55.794530       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1013 08:44:55.794609       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1013 08:44:56.028104       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1013 08:44:56.028250       1 server_linux.go:132] "Using iptables Proxier"
I1013 08:44:56.040621       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1013 08:44:56.061963       1 server.go:527] "Version info" version="v1.34.0"
I1013 08:44:56.062607       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 08:44:56.067328       1 config.go:200] "Starting service config controller"
I1013 08:44:56.073228       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1013 08:44:56.069144       1 config.go:403] "Starting serviceCIDR config controller"
I1013 08:44:56.079173       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1013 08:44:56.079184       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1013 08:44:56.069132       1 config.go:106] "Starting endpoint slice config controller"
I1013 08:44:56.079192       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1013 08:44:56.079194       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1013 08:44:56.068882       1 config.go:309] "Starting node config controller"
I1013 08:44:56.083691       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1013 08:44:56.083703       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1013 08:44:56.177513       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [e23019ca55e8] <==
I1013 04:34:25.773585       1 serving.go:386] Generated self-signed cert in-memory
W1013 04:34:38.370277       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1013 04:34:38.384325       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1013 04:34:38.385681       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1013 04:34:38.385720       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1013 04:34:39.196458       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1013 04:34:39.205271       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 04:34:39.300516       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1013 04:34:39.319278       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1013 04:34:39.332227       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 04:34:39.341060       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 04:34:39.559973       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1013 06:52:03.718705       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?allowWatchBookmarks=true&resourceVersion=10016&timeout=7m51s&timeoutSeconds=471&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1013 06:52:03.718954       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?allowWatchBookmarks=true&resourceVersion=10131&timeout=7m51s&timeoutSeconds=471&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1013 06:52:03.737278       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?allowWatchBookmarks=true&resourceVersion=10141&timeout=6m17s&timeoutSeconds=377&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1013 06:52:03.737344       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?allowWatchBookmarks=true&resourceVersion=10140&timeout=7m1s&timeoutSeconds=421&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1013 06:52:03.737377       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?allowWatchBookmarks=true&resourceVersion=10098&timeout=9m19s&timeoutSeconds=559&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1013 06:52:03.737403       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceclaims?allowWatchBookmarks=true&resourceVersion=10011&timeout=8m56s&timeoutSeconds=536&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1013 06:52:03.737461       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/nodes?allowWatchBookmarks=true&resourceVersion=10252&timeout=7m43s&timeoutSeconds=463&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1013 06:52:03.737507       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?allowWatchBookmarks=true&resourceVersion=10137&timeout=6m33s&timeoutSeconds=393&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1013 06:52:03.737537       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=10351&timeout=9m46s&timeoutSeconds=586&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1013 06:52:03.866693       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=10056&timeout=6m39s&timeoutSeconds=399&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1013 06:52:03.867120       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?allowWatchBookmarks=true&resourceVersion=10328&timeout=8m38s&timeoutSeconds=518&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1013 06:52:03.867810       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/deviceclasses?allowWatchBookmarks=true&resourceVersion=10071&timeout=5m19s&timeoutSeconds=319&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1013 06:52:03.867881       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/resource.k8s.io/v1/resourceslices?allowWatchBookmarks=true&resourceVersion=10106&timeout=8m54s&timeoutSeconds=534&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1013 06:52:03.867998       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?allowWatchBookmarks=true&resourceVersion=10066&timeout=5m44s&timeoutSeconds=344&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1013 06:52:03.868120       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?allowWatchBookmarks=true&resourceVersion=10012&timeout=7m2s&timeoutSeconds=422&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1013 06:52:03.868152       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=10019&timeout=9m40s&timeoutSeconds=580&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1013 06:52:03.868268       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=10128&timeout=6m13s&timeoutSeconds=373&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1013 06:52:03.868296       1 reflector.go:205] "Failed to watch" err="Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?allowWatchBookmarks=true&resourceVersion=10059&timeout=5m22s&timeoutSeconds=322&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"


==> kube-scheduler [fb9b2f56572c] <==
I1013 08:44:47.509000       1 serving.go:386] Generated self-signed cert in-memory
W1013 08:44:50.433679       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1013 08:44:50.434104       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1013 08:44:50.434545       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1013 08:44:50.435092       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1013 08:44:50.543832       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1013 08:44:50.543857       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 08:44:50.568467       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1013 08:44:50.572154       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 08:44:50.578369       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 08:44:50.581068       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1013 08:44:50.780275       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.414605    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.463136    1291 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: Get \"https://192.168.49.2:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
Oct 13 08:44:45 minikube kubelet[1291]: I1013 08:44:45.515909    1291 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.516400    1291 kubelet_node_status.go:107] "Unable to register node with API server" err="Post \"https://192.168.49.2:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.516753    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:45 minikube kubelet[1291]: I1013 08:44:45.606191    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f10737398bad5ba3d1578f10df47aa07d6b51fc3a5b4dc677e5eb2c9d4edaf41"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.619901    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:45 minikube kubelet[1291]: I1013 08:44:45.627157    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3bb8ea08d460110df8c6c4f4315d476a711efc89a9aa4c01129aaf2a6026085b"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.628469    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.722315    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.824351    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:45 minikube kubelet[1291]: E1013 08:44:45.926152    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.027309    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.112893    1291 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.128795    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.230740    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.331512    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.433255    1291 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.461413    1291 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.678539    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.721828    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.743853    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:46 minikube kubelet[1291]: E1013 08:44:46.761481    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:48 minikube kubelet[1291]: E1013 08:44:48.221765    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:48 minikube kubelet[1291]: E1013 08:44:48.221938    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:48 minikube kubelet[1291]: E1013 08:44:48.222146    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:48 minikube kubelet[1291]: E1013 08:44:48.223114    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:48 minikube kubelet[1291]: I1013 08:44:48.721423    1291 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Oct 13 08:44:49 minikube kubelet[1291]: E1013 08:44:49.234700    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:49 minikube kubelet[1291]: E1013 08:44:49.235757    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:50 minikube kubelet[1291]: E1013 08:44:50.259347    1291 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.424649    1291 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.673829    1291 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.673942    1291 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.673982    1291 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.675504    1291 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 13 08:44:50 minikube kubelet[1291]: E1013 08:44:50.726057    1291 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.726190    1291 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: E1013 08:44:50.772544    1291 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.772584    1291 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: E1013 08:44:50.798720    1291 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: I1013 08:44:50.798782    1291 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Oct 13 08:44:50 minikube kubelet[1291]: E1013 08:44:50.809545    1291 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 13 08:44:51 minikube kubelet[1291]: I1013 08:44:51.223940    1291 apiserver.go:52] "Watching apiserver"
Oct 13 08:44:51 minikube kubelet[1291]: I1013 08:44:51.325642    1291 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Oct 13 08:44:51 minikube kubelet[1291]: I1013 08:44:51.326156    1291 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6afee95b-d43c-45a2-8acf-ef5335c0ab4e-lib-modules\") pod \"kube-proxy-gj295\" (UID: \"6afee95b-d43c-45a2-8acf-ef5335c0ab4e\") " pod="kube-system/kube-proxy-gj295"
Oct 13 08:44:51 minikube kubelet[1291]: I1013 08:44:51.326370    1291 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/5ab2c28c-6f29-4327-9de7-54911c96f427-tmp\") pod \"storage-provisioner\" (UID: \"5ab2c28c-6f29-4327-9de7-54911c96f427\") " pod="kube-system/storage-provisioner"
Oct 13 08:44:51 minikube kubelet[1291]: I1013 08:44:51.326424    1291 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6afee95b-d43c-45a2-8acf-ef5335c0ab4e-xtables-lock\") pod \"kube-proxy-gj295\" (UID: \"6afee95b-d43c-45a2-8acf-ef5335c0ab4e\") " pod="kube-system/kube-proxy-gj295"
Oct 13 08:44:52 minikube kubelet[1291]: I1013 08:44:52.367365    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="18a57cc7d1dbea6bfd791cb064d60daecaf00b0e8fa532364ef25c0c6e4668cf"
Oct 13 08:44:54 minikube kubelet[1291]: I1013 08:44:54.316431    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="12cbfc8655ca92cda3176a78910984dc64f0ae3febb5b8e2bd776b31cd82dabd"
Oct 13 08:44:54 minikube kubelet[1291]: I1013 08:44:54.357664    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c825129e35d4c0403773157f0ec9212637840876af59146eb435e06ba05c57de"
Oct 13 08:44:54 minikube kubelet[1291]: I1013 08:44:54.382495    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="441e9d0608e2dbdec9f9363f06718dc168e60b2caa99c290346cb1ecf1fb0415"
Oct 13 08:44:55 minikube kubelet[1291]: I1013 08:44:55.157267    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d6f08c3cc497182fbf0025655c92d323435928459055b3eb02626125dc929c41"
Oct 13 08:44:55 minikube kubelet[1291]: I1013 08:44:55.192349    1291 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3e074ede66409f1f3da66a076849d355f9e5f692f9bd531d8e912ce7127cae4f"
Oct 13 08:44:56 minikube kubelet[1291]: I1013 08:44:56.250793    1291 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 13 08:44:58 minikube kubelet[1291]: I1013 08:44:58.385052    1291 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 13 08:45:25 minikube kubelet[1291]: I1013 08:45:25.617104    1291 scope.go:117] "RemoveContainer" containerID="a0a00e330395e5e287c1323c1519170be5f9b4a97e6842800c9942607cabfb6b"
Oct 13 08:45:25 minikube kubelet[1291]: I1013 08:45:25.617623    1291 scope.go:117] "RemoveContainer" containerID="0d4616cf8c4a75c3430ca088fc2645b6d1541cf621cde41536e33176c53d3c07"
Oct 13 08:45:25 minikube kubelet[1291]: E1013 08:45:25.617784    1291 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(5ab2c28c-6f29-4327-9de7-54911c96f427)\"" pod="kube-system/storage-provisioner" podUID="5ab2c28c-6f29-4327-9de7-54911c96f427"
Oct 13 08:45:39 minikube kubelet[1291]: I1013 08:45:39.422976    1291 scope.go:117] "RemoveContainer" containerID="0d4616cf8c4a75c3430ca088fc2645b6d1541cf621cde41536e33176c53d3c07"


==> storage-provisioner [0d4616cf8c4a] <==
I1013 08:44:54.987455       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1013 08:45:25.098973       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [32f93fae00aa] <==
I1013 08:45:41.371421       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1013 08:45:41.545389       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1013 08:45:41.551470       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
W1013 08:45:41.576291       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:45.061411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:49.427256       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:53.037429       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:56.184852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:59.227588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:59.282992       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
I1013 08:45:59.287324       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1013 08:45:59.312817       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c4395bb8-bb8a-4a6c-865c-c523f0362611!
I1013 08:45:59.288139       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"304398d4-d43f-43ec-840e-2ada391b3ebd", APIVersion:"v1", ResourceVersion:"13013", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c4395bb8-bb8a-4a6c-865c-c523f0362611 became leader
W1013 08:45:59.465514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:45:59.498412       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
I1013 08:46:00.373214       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c4395bb8-bb8a-4a6c-865c-c523f0362611!
W1013 08:46:01.511933       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:01.550773       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:03.569249       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:03.629625       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:05.668186       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:05.767056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:07.818749       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:07.849311       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:09.857697       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:09.870602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:11.881376       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:11.894388       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:13.919688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:13.934723       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:16.276812       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:16.295257       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:18.304023       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:18.332260       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:20.352435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:20.376989       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:22.397514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:22.409639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:24.435116       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:24.465231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:26.488975       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:26.704502       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:28.720546       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:28.742886       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:30.751778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:30.775565       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:32.793749       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:32.803123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:34.810928       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1013 08:46:34.826264       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

